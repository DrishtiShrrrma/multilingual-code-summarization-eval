{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets nltk rouge-score sacrebleu sentence-transformers sentencepiece fsspec==2025.3.2 bert-score --quiet\n",
        "!pip install indic-nlp-library camel-tools"
      ],
      "metadata": {
        "trusted": true,
        "id": "_HobtJ41zZYk",
        "execution": {
          "iopub.status.idle": "2025-05-17T15:48:56.062775Z",
          "shell.execute_reply.started": "2025-05-17T15:45:40.577563Z",
          "shell.execute_reply": "2025-05-17T15:48:56.062039Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unbabel-comet\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "Pqv2DWjy3eo5",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "jfXr1epW3nY8"
      },
      "cell_type": "code",
      "source": [
        "# ─── IMPORTS ─────────────────────────────────────────────\n",
        "import os\n",
        "import json\n",
        "import hashlib\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "import sacrebleu\n",
        "from sentence_transformers import util\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel\n",
        "from bert_score import score as bert_score\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from indicnlp.tokenize.indic_tokenize import trivial_tokenize\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from evaluate import load as evaluate_load"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "1gl36JDN3nY8"
      },
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gdown\n",
        "!gdown --folder 1QdxrYnelt9poi45eLT5xgObihDRb_OtV -O /content/103080"
      ],
      "metadata": {
        "id": "mtiJQX5wbiU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone updated repo\n",
        "!git clone https://github.com/DrishtiShrrrma/nueva.git\n",
        "\n",
        "# Adjust base_dir to new path for prompt-based summaries\n",
        "base_dir = \"/content/nueva/prompt_analysis\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msS29R9Ebx0d",
        "outputId": "d8fba6b1-e64b-4233-89dc-278bd206baa5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'codeclarity'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── CONFIGURATION ──────────────────────────────────────────────\n",
        "backtranslation_dir = \"backtranslations_cache\"\n",
        "os.makedirs(backtranslation_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# Mapping for summary field name → Display name\n",
        "json_field_to_lang = {\n",
        "    \"chinese\":     \"Chinese\",\n",
        "    \"french\":      \"French\",\n",
        "    \"spanish\":     \"Spanish\",\n",
        "    \"portuguese\":  \"Portuguese\",\n",
        "    \"arabic\":      \"Arabic\",\n",
        "    \"hindi\":       \"Hindi\"\n",
        "}\n",
        "\n",
        "# Mapping for Display name → M2M-100 language code (used for backtranslation)\n",
        "bt_lang_code_map = {\n",
        "    \"Chinese\":     \"zh\",\n",
        "    \"French\":      \"fr\",\n",
        "    \"Spanish\":     \"es\",\n",
        "    \"Portuguese\":  \"pt\",\n",
        "    \"Arabic\":      \"ar\",\n",
        "    \"Hindi\":       \"hi\"\n",
        "}\n",
        "\n",
        "\n",
        "# Load M2M-100 model\n",
        "bt_model_name = \"facebook/m2m100_418M\"\n",
        "bt_model_tag  = bt_model_name.split(\"/\")[-1]\n",
        "bt_tokenizer  = AutoTokenizer.from_pretrained(bt_model_name)\n",
        "bt_model      = AutoModelForSeq2SeqLM.from_pretrained(bt_model_name)\n",
        "device        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "bt_model      = bt_model.to(device)\n",
        "\n",
        "\n",
        "# Caches\n",
        "embedding_model     = None\n",
        "bertscore_model     = None\n",
        "bertscore_tokenizer = None\n",
        "side_tokenizer      = None\n",
        "side_model          = None"
      ],
      "metadata": {
        "id": "tK2Tska4XGQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── BACK-TRANSLATION FUNCTION ─────────────────────────────────────────────\n",
        "def bt_function(text, src_lang_name):\n",
        "    key = f\"{src_lang_name}_{bt_model_tag}_{hashlib.md5(text.encode()).hexdigest()}\"\n",
        "    cache_file = os.path.join(backtranslation_dir, key + \".txt\")\n",
        "    if os.path.exists(cache_file):\n",
        "        return open(cache_file, 'r', encoding='utf-8').read()\n",
        "\n",
        "    src_lang = bt_lang_code_map.get(src_lang_name)\n",
        "    tgt_lang = \"en\"  # Always translating into English\n",
        "\n",
        "    if src_lang is None:\n",
        "        return text\n",
        "\n",
        "    bt_tokenizer.src_lang = src_lang\n",
        "    inputs = bt_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    output_ids = bt_model.generate(**inputs, forced_bos_token_id=bt_tokenizer.get_lang_id(tgt_lang))\n",
        "    output_text = bt_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    with open(cache_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(output_text)\n",
        "\n",
        "    return output_text\n",
        "\n",
        "\n",
        "# ─── METRIC FUNCTIONS ──────────────────────────────────────────────────────\n",
        "def compute_bertscore(refs, hyps):\n",
        "    P, R, F1 = bert_score(\n",
        "        hyps,\n",
        "        refs,\n",
        "        model_type=\"xlm-roberta-large\",\n",
        "        lang=\"en\",\n",
        "        rescale_with_baseline=False\n",
        "    )\n",
        "    return {\n",
        "        \"precision\": round(P.mean().item(), 4),\n",
        "        \"recall\":    round(R.mean().item(), 4),\n",
        "        \"f1\":        round(F1.mean().item(), 4)\n",
        "    }"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T15:51:55.056893Z",
          "iopub.execute_input": "2025-05-17T15:51:55.057441Z",
          "iopub.status.idle": "2025-05-17T15:52:05.406210Z",
          "shell.execute_reply.started": "2025-05-17T15:51:55.057417Z",
          "shell.execute_reply": "2025-05-17T15:52:05.405181Z"
        },
        "id": "48Z-T6bgzZYp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── MEAN POOLING (for SIDE) ───────────────────────────────────────────────\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0]\n",
        "    mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * mask, 1) / torch.clamp(mask.sum(1), min=1e-9)\n",
        "\n",
        "\n",
        "def compute_side_score(codes, hyps):\n",
        "    global side_tokenizer, side_model\n",
        "    if side_model is None:\n",
        "        checkpoint = \"/content/103080\"\n",
        "        side_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "        side_model     = AutoModel.from_pretrained(checkpoint)\n",
        "        if torch.cuda.is_available(): side_model = side_model.cuda()\n",
        "        side_model.eval()\n",
        "    scores = []\n",
        "    for code, summ in zip(codes, hyps):\n",
        "        enc = side_tokenizer([code, summ], padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        if torch.cuda.is_available(): enc = {k:v.cuda() for k,v in enc.items()}\n",
        "        with torch.no_grad(): out = side_model(**enc)\n",
        "        pooled = mean_pooling(out, enc['attention_mask'])\n",
        "        normed = torch.nn.functional.normalize(pooled, p=2, dim=1)\n",
        "        scores.append(util.pytorch_cos_sim(normed[0], normed[1]).item())\n",
        "    return round(float(np.mean(scores)),4)\n",
        "\n",
        "def compute_meteor_score(refs, hyps):\n",
        "    sc = []\n",
        "    for r,h in zip(refs, hyps):\n",
        "        rt = word_tokenize(r.lower()); ht = word_tokenize(h.lower())\n",
        "        sc.append(meteor_score([rt], ht))\n",
        "    return round(float(np.mean(sc)),4)\n",
        "\n",
        "def compute_chrf_score(refs, hyps):\n",
        "  refs = [r.lower() for r in refs]\n",
        "  hyps = [h.lower() for h in hyps]\n",
        "\n",
        "  res = sacrebleu.corpus_chrf(hyps, [refs], word_order=2)\n",
        "  return round(res.score / 100, 4)\n",
        "\n",
        "## ----BLEU METRIC-----------\n",
        "def compute_bleu_sacre(refs, hyps, lang_name):\n",
        "    lang_name = lang_name.lower()\n",
        "\n",
        "    # Define tokenizer per language\n",
        "    tokenizer_map = {\n",
        "        \"chinese\": \"zh\",\n",
        "        \"french\": \"13a\",\n",
        "        \"portuguese\": \"13a\",\n",
        "        \"arabic\": \"intl\",\n",
        "        \"hindi\": \"intl\",\n",
        "        \"spanish\": \"13a\"\n",
        "    }\n",
        "\n",
        "    # Default tokenizer if language not found\n",
        "    tokenizer = tokenizer_map.get(lang_name, \"13a\")\n",
        "\n",
        "    # Compute BLEU-4\n",
        "    score = sacrebleu.corpus_bleu(hyps, [refs], tokenize=tokenizer)\n",
        "    return round(score.score / 100, 4) # Normalize to 0–1 like nltk\n",
        "\n",
        "\n",
        "\n",
        "def tokenize(text, lang):\n",
        "    lang = lang.lower()\n",
        "    if lang == \"chinese\":\n",
        "        return list(text.strip())\n",
        "    elif lang == \"arabic\":\n",
        "        return simple_word_tokenize(text)\n",
        "    elif lang == \"hindi\":\n",
        "        return trivial_tokenize(text, lang='hi')\n",
        "    elif lang in [\"french\", \"portuguese\"]:\n",
        "        return word_tokenize(text, language=lang)\n",
        "    else:\n",
        "        return text.strip().split()\n",
        "\n",
        "\n",
        "def compute_bleu_nltk(refs_tokenized, hyps_tokenized):\n",
        "    smoothie = SmoothingFunction().method1\n",
        "    score = corpus_bleu(\n",
        "        refs_tokenized,\n",
        "        hyps_tokenized,\n",
        "        weights=(0.25, 0.25, 0.25, 0.25),\n",
        "        smoothing_function=smoothie\n",
        "    )\n",
        "    return round(score, 4)\n",
        "\n",
        "## ----COMET METRIC-----------\n",
        "\n",
        "comet = evaluate_load(\"comet\", config_name=\"Unbabel/wmt22-comet-da\")\n",
        "\n",
        "def compute_comet_score(sources, references, hypotheses, batch_size=8, gpus=0):\n",
        "    result = comet.compute(\n",
        "        sources=sources,\n",
        "        predictions=hypotheses,\n",
        "        references=references,\n",
        "    )\n",
        "    per_example = result.get(\"scores\", [])\n",
        "    mean_score = float(np.mean(per_example)) if per_example else 0.0\n",
        "    return round(mean_score, 4), per_example\n",
        "\n",
        "\n",
        "# COMPUTE ALL METRICS\n",
        "def compute_all_metrics(codes, refs, hyps, lang_name, code_lang):\n",
        "    print(f\"  Computing backtranslation-based metrics for {lang_name}...\")\n",
        "    bt = [bt_function(h, lang_name) for h in hyps]\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    refs_tokenized = [[tokenize(r, lang_name)] for r in refs]\n",
        "    hyps_tokenized = [tokenize(b, lang_name) for b in bt]\n",
        "\n",
        "    # Compute BLEU using tokenized inputs\n",
        "    bleu_nltk = compute_bleu_nltk(refs_tokenized, hyps_tokenized)\n",
        "\n",
        "    bleu_sacre = compute_bleu_sacre(refs, bt, lang_name)\n",
        "    bleu_diff = round(abs(bleu_nltk - bleu_sacre), 4)\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    rl = [scorer.score(r, b)['rougeL'].fmeasure for r, b in zip(refs, bt)]\n",
        "    comet_mean, comet_per_example = compute_comet_score(\n",
        "        sources=hyps,\n",
        "        references=refs,\n",
        "        hypotheses=bt\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"bleu4_nltk\": round(bleu_nltk, 4),\n",
        "        \"bleu4_sacrebleu\": bleu_sacre,\n",
        "        \"bleu4_diff\": bleu_diff,\n",
        "        \"rougeL\": round(np.mean(rl), 4),\n",
        "        \"meteor\": compute_meteor_score(refs, bt),\n",
        "        \"chrf++\": compute_chrf_score(refs, bt),\n",
        "        \"side_bt\": compute_side_score(codes, bt),\n",
        "        \"comet_mean\": comet_mean,\n",
        "        \"comet_per_example\": comet_per_example\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T16:01:07.499915Z",
          "iopub.execute_input": "2025-05-17T16:01:07.500259Z"
        },
        "id": "FLA_b7_XzZYq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "km-X3KQa3nY9"
      },
      "cell_type": "code",
      "source": [
        "# ─── MAIN EVALUATION ─────────────────────────────────────\n",
        "def run_evaluation():\n",
        "    all_results = []\n",
        "    base_dir = \"/content/nueva/prompt_analysis\"\n",
        "    model_folders = [\"codegemma\", \"gemma-2-9b-it\", \"qwen2.5coder\", \"deepseekcoder\"]\n",
        "    prompt_subdirs = [\"prompt0\", \"prompt1\", \"prompt2\", \"prompt3\"]\n",
        "\n",
        "    for model_folder in model_folders:\n",
        "        for prompt in prompt_subdirs:\n",
        "            prompt_path = os.path.join(base_dir, model_folder, prompt)\n",
        "            if not os.path.isdir(prompt_path):\n",
        "                continue\n",
        "\n",
        "            for fname in os.listdir(prompt_path):\n",
        "                if not fname.endswith(\".json\") or not fname.startswith(\"all_languages_prompt\"):\n",
        "                    continue\n",
        "\n",
        "                summary_path = os.path.join(prompt_path, fname)\n",
        "                print(f\"\\nProcessing file: {summary_path}\")\n",
        "\n",
        "                with open(summary_path, encoding='utf-8') as f:\n",
        "                    data = json.load(f)\n",
        "\n",
        "                if not data:\n",
        "                    print(\"  Skipped: empty file\")\n",
        "                    continue\n",
        "\n",
        "                codes = [d.get(\"code\", \"\") for d in data]\n",
        "                refs = [d.get(\"summary_english\", d.get(\"docstring\", \"\")) for d in data]\n",
        "                code_lang = data[0].get(\"language\", \"unknown\")\n",
        "                model_name = data[0].get(\"model_name\", model_folder)\n",
        "                prompt_used = data[0].get(\"prompt_used\", prompt)\n",
        "\n",
        "                for field, lang_name in json_field_to_lang.items():\n",
        "                    hyp_key = f\"summary_{field}\"\n",
        "                    if hyp_key not in data[0]:\n",
        "                        print(f\"  Skipping {lang_name} — {hyp_key} not found.\")\n",
        "                        continue\n",
        "\n",
        "                    hyps = [d.get(hyp_key, \"\") for d in data]\n",
        "                    if not any(hyps):\n",
        "                        print(f\"  Skipping {lang_name} — all summaries empty.\")\n",
        "                        continue\n",
        "\n",
        "                    print(f\"  → Evaluating summaries in {lang_name}...\")\n",
        "                    bert = compute_bertscore(refs, hyps)\n",
        "                    side_original = compute_side_score(codes, hyps)\n",
        "                    metrics = compute_all_metrics(codes, refs, hyps, lang_name, code_lang)\n",
        "                    side_drop = round(side_original - metrics[\"side_bt\"], 4)\n",
        "\n",
        "                    for i, entry in enumerate(data):\n",
        "                        code = entry.get(\"code\", \"\")\n",
        "                        sample_id = entry.get(\"id\", f\"{code_lang}_{i}\")\n",
        "                        full_func = entry.get(\"whole_func_string\", code)\n",
        "                        word_len = len(full_func.strip().split())\n",
        "\n",
        "                        generated_summary = entry.get(hyp_key, \"\")\n",
        "                        backtranslated_summary = bt_function(generated_summary, lang_name)\n",
        "\n",
        "                        result = {\n",
        "                            \"sample_id\": sample_id,\n",
        "                            \"model_folder_name\": model_folder,\n",
        "                            \"model_name\": model_name,\n",
        "                            \"programming_language\": code_lang,\n",
        "                            \"language\": lang_name,\n",
        "                            \"prompt_used\": prompt_used,\n",
        "                            \"bt_model\": bt_model_tag,\n",
        "                            \"word_len\": word_len,\n",
        "                            \"length_bucket\": entry.get(\"length_bucket\", \"unknown\"),\n",
        "                            \"reference_summary\": entry.get(\"summary_english\", entry.get(\"docstring\", \"\")),\n",
        "                            \"generated_summary\": generated_summary,\n",
        "                            \"backtranslated_summary\": backtranslated_summary,\n",
        "                            \"bertscore_f1\": bert[\"f1\"],\n",
        "                            \"bertscore_precision\": bert[\"precision\"],\n",
        "                            \"bertscore_recall\": bert[\"recall\"],\n",
        "                            \"side_original\": side_original,\n",
        "                            \"side_bt\": metrics[\"side_bt\"],\n",
        "                            \"side_drop\": side_drop,\n",
        "                            \"bleu4_nltk\": metrics[\"bleu4_nltk\"],\n",
        "                            \"bleu4_sacrebleu\": metrics[\"bleu4_sacrebleu\"],\n",
        "                            \"bleu4_diff\": metrics[\"bleu4_diff\"],\n",
        "                            \"rougeL\": metrics[\"rougeL\"],\n",
        "                            \"meteor\": metrics[\"meteor\"],\n",
        "                            \"chrf++\": metrics[\"chrf++\"],\n",
        "                            \"comet_mean\": metrics[\"comet_mean\"],\n",
        "                            \"comet_example_score\": metrics[\"comet_per_example\"][i] if i < len(metrics[\"comet_per_example\"]) else None\n",
        "                        }\n",
        "\n",
        "                        all_results.append(result)\n",
        "\n",
        "    # Save results\n",
        "    os.makedirs(backtranslation_dir, exist_ok=True)\n",
        "    json_out = os.path.join(backtranslation_dir, f\"all_scores_bt_{bt_model_tag}.json\")\n",
        "    csv_out = os.path.join(backtranslation_dir, f\"all_scores_bt_{bt_model_tag}.csv\")\n",
        "\n",
        "    with open(json_out, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(all_results, f, indent=2)\n",
        "\n",
        "    pd.DataFrame(all_results).to_csv(csv_out, index=False)\n",
        "\n",
        "    print(f\"\\n✅ Saved results to:\\n  JSON: {json_out}\\n  CSV:  {csv_out}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}