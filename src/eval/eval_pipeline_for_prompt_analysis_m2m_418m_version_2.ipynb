{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets nltk rouge-score sacrebleu sentence-transformers sentencepiece fsspec==2025.3.2 bert-score --quiet\n",
        "!pip install indic-nlp-library camel-tools"
      ],
      "metadata": {
        "trusted": true,
        "id": "_HobtJ41zZYk",
        "outputId": "6d7f311a-ee53-40a3-c783-aa41e089e175",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.idle": "2025-05-17T15:48:56.062775Z",
          "shell.execute_reply.started": "2025-05-17T15:45:40.577563Z",
          "shell.execute_reply": "2025-05-17T15:48:56.062039Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting camel-tools\n",
            "  Downloading camel_tools-1.5.6-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting sphinx-argparse (from indic-nlp-library)\n",
            "  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting morfessor (from indic-nlp-library)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (2.0.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.17.0)\n",
            "Collecting docopt (from camel-tools)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from camel-tools) (5.5.2)\n",
            "Collecting numpy (from indic-nlp-library)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.6.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from camel-tools) (0.3.7)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from camel-tools) (2.6.0+cu124)\n",
            "Collecting transformers<4.44.0,>=4.0 (from camel-tools)\n",
            "  Downloading transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (from camel-tools) (0.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from camel-tools) (2.32.3)\n",
            "Collecting emoji (from camel-tools)\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting pyrsistent (from camel-tools)\n",
            "  Downloading pyrsistent-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from camel-tools) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from camel-tools) (4.67.1)\n",
            "Collecting muddler (from camel-tools)\n",
            "  Downloading muddler-0.1.3-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting camel-kenlm>=2025.4.8 (from camel-tools)\n",
            "  Downloading camel-kenlm-2025.4.8.zip (556 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m556.5/556.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->camel-tools) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.33.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.5.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers<4.44.0,>=4.0->camel-tools)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->camel-tools) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->camel-tools) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->camel-tools) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->camel-tools) (2025.6.15)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->camel-tools) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->camel-tools) (3.6.0)\n",
            "Requirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (8.2.3)\n",
            "Requirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers<4.44.0,>=4.0->camel-tools) (1.1.5)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.19.2)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.1)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->camel-tools) (3.0.2)\n",
            "Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading camel_tools-1.5.6-py3-none-any.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m115.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.43.4-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m130.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Downloading muddler-0.1.3-py3-none-any.whl (16 kB)\n",
            "Downloading pyrsistent-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (120 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\n",
            "Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m134.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: camel-kenlm, docopt\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unbabel-comet\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "Pqv2DWjy3eo5",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "jfXr1epW3nY8"
      },
      "cell_type": "code",
      "source": [
        "# ─── IMPORTS ─────────────────────────────────────────────\n",
        "import os\n",
        "import json\n",
        "import hashlib\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "import sacrebleu\n",
        "from sentence_transformers import util\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel\n",
        "from bert_score import score as bert_score\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from indicnlp.tokenize.indic_tokenize import trivial_tokenize\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from evaluate import load as evaluate_load"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "1gl36JDN3nY8"
      },
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gdown\n",
        "!gdown --folder 1QdxrYnelt9poi45eLT5xgObihDRb_OtV -O /content/103080"
      ],
      "metadata": {
        "id": "mtiJQX5wbiU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone updated repo\n",
        "!git clone https://github.com/DrishtiShrrrma/nueva.git\n",
        "\n",
        "# Adjust base_dir to new path for prompt-based summaries\n",
        "base_dir = \"/content/nueva/prompt_analysis\"\n",
        "\n"
      ],
      "metadata": {
        "id": "msS29R9Ebx0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "qNF0DK-YqY5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── CONFIGURATION ──────────────────────────────────────────────\n",
        "backtranslation_dir = \"backtranslations_cache\"\n",
        "os.makedirs(backtranslation_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# Mapping for summary field name → Display name\n",
        "json_field_to_lang = {\n",
        "    \"chinese\":     \"Chinese\",\n",
        "    \"french\":      \"French\",\n",
        "    \"spanish\":     \"Spanish\",\n",
        "    \"portuguese\":  \"Portuguese\",\n",
        "    \"arabic\":      \"Arabic\",\n",
        "    \"hindi\":       \"Hindi\"\n",
        "}\n",
        "\n",
        "# Mapping for Display name → M2M-100 language code (used for backtranslation)\n",
        "bt_lang_code_map = {\n",
        "    \"Chinese\":     \"zh\",\n",
        "    \"French\":      \"fr\",\n",
        "    \"Spanish\":     \"es\",\n",
        "    \"Portuguese\":  \"pt\",\n",
        "    \"Arabic\":      \"ar\",\n",
        "    \"Hindi\":       \"hi\"\n",
        "}\n",
        "\n",
        "\n",
        "# Load llamax model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "bt_model_name = \"facebook/m2m100_418M\"\n",
        "bt_model_tag = \"m2m100_418M\"\n",
        "\n",
        "\n",
        "is_encoder_decoder = \"m2m\" in bt_model_name.lower() or \"opus\" in bt_model_name.lower()\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "bt_tokenizer = AutoTokenizer.from_pretrained(bt_model_name)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "if is_encoder_decoder:\n",
        "    bt_model = AutoModelForSeq2SeqLM.from_pretrained(bt_model_name).to(device)\n",
        "else:\n",
        "    bt_model = AutoModelForCausalLM.from_pretrained(bt_model_name, torch_dtype=torch.bfloat16 if \"tower\" in bt_model_name.lower() else None).to(device)\n",
        "bt_model.eval()\n",
        "\n",
        "\n",
        "\n",
        "# Caches\n",
        "embedding_model     = None\n",
        "bertscore_model     = None\n",
        "bertscore_tokenizer = None\n",
        "side_tokenizer      = None\n",
        "side_model          = None"
      ],
      "metadata": {
        "id": "tK2Tska4XGQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sanitize_text(text: str) -> str:\n",
        "    for token in [\n",
        "        \"<|end_of_text|>\", \"</s>\", \"<s>\", \"<|eot_id|>\",\n",
        "        \"<|im_start|>user\", \"<|im_start|>assistant\", \"<|im_start|>system\", \"<|im_end|>\",\n",
        "        \"<|user|>\", \"<|assistant|>\", \"<|system|>\",\n",
        "        \"<|CHATBOT_TOKEN|>\", \"<|START_OF_TURN_TOKEN|>\", \"<|END_OF_TURN_TOKEN|>\", \"<BOS_TOKEN>\"\n",
        "    ]:\n",
        "        text = text.replace(token, \"\")\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "\n",
        "def clean_translation_output(decoded: str):\n",
        "    decoded = sanitize_text(decoded)\n",
        "\n",
        "    # Handle known prompt-style artifacts\n",
        "    for token in [\"### Response:\", \"<|CHATBOT_TOKEN|>\", \"English:\", \"Translation:\"]:\n",
        "        if token in decoded:\n",
        "            decoded = decoded.split(token, 1)[-1].strip()\n",
        "\n",
        "    # Handle chat-style output\n",
        "    if \"<|im_start|> assistant\" in decoded:\n",
        "        decoded = decoded.split(\"<|im_start|> assistant\", 1)[-1].strip()\n",
        "    elif \"<|im_start|>\" in decoded:\n",
        "        decoded = decoded.split(\"<|im_start|>\", 1)[-1].strip()\n",
        "\n",
        "    if \"<|im_end|>\" in decoded:\n",
        "        decoded = decoded.split(\"<|im_end|>\", 1)[0].strip()\n",
        "\n",
        "    return decoded.strip()\n"
      ],
      "metadata": {
        "id": "QxP0HPFvrmNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_template(text, src_lang, tgt_lang):\n",
        "    return f\"Translate the following text from {src_lang} to {tgt_lang}:\\n{text.strip()}\\nTranslation:\"\n",
        "\n",
        "def bt_function(text, src_lang_name):\n",
        "    src_lang_name = src_lang_name.strip()\n",
        "    key = f\"{src_lang_name}_{bt_model_tag}_{hashlib.md5(text.encode()).hexdigest()}\"\n",
        "    cache_file = os.path.join(backtranslation_dir, key + \".txt\")\n",
        "    if os.path.exists(cache_file):\n",
        "        return open(cache_file, 'r', encoding='utf-8').read()\n",
        "\n",
        "    # Normalize names\n",
        "    src_lang_key = src_lang_name.lower()\n",
        "    src_lang = json_field_to_lang.get(src_lang_key, src_lang_name)\n",
        "    tgt_lang = \"English\"\n",
        "\n",
        "    is_encoder_decoder = isinstance(bt_model, AutoModelForSeq2SeqLM)\n",
        "\n",
        "    if is_encoder_decoder:\n",
        "        tgt_lang_code = \"en\"\n",
        "        # FIXED: Use capitalized version to match the map keys\n",
        "        src_lang_code = bt_lang_code_map.get(src_lang_name.strip().capitalize())\n",
        "        if not src_lang_code:\n",
        "            return text\n",
        "        bt_tokenizer.src_lang = src_lang_code\n",
        "        inputs = bt_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        output_ids = bt_model.generate(**inputs, forced_bos_token_id=bt_tokenizer.get_lang_id(tgt_lang_code))\n",
        "        output = bt_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    else:\n",
        "        prompt = prompt_template(text, src_lang, tgt_lang)\n",
        "        if hasattr(bt_tokenizer, \"apply_chat_template\"):\n",
        "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "            prompt = bt_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        inputs = bt_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            output_ids = bt_model.generate(\n",
        "                inputs.input_ids,\n",
        "                max_new_tokens=512,\n",
        "                pad_token_id=bt_tokenizer.eos_token_id,\n",
        "                do_sample=False\n",
        "            )\n",
        "        decoded = bt_tokenizer.decode(output_ids[0], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
        "        output = clean_translation_output(decoded)\n",
        "\n",
        "    output = output.replace(\"<|end_of_text|>\", \"\").strip()\n",
        "    with open(cache_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(output)\n",
        "\n",
        "    return sanitize_text(output)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ─── METRIC FUNCTIONS ──────────────────────────────────────────────────────\n",
        "def compute_bertscore(refs, hyps):\n",
        "    P, R, F1 = bert_score(\n",
        "        hyps,\n",
        "        refs,\n",
        "        model_type=\"xlm-roberta-large\",\n",
        "        lang=\"en\",\n",
        "        rescale_with_baseline=False\n",
        "    )\n",
        "    return {\n",
        "        \"precision\": round(P.mean().item(), 4),\n",
        "        \"recall\":    round(R.mean().item(), 4),\n",
        "        \"f1\":        round(F1.mean().item(), 4)\n",
        "    }"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T15:51:55.056893Z",
          "iopub.execute_input": "2025-05-17T15:51:55.057441Z",
          "iopub.status.idle": "2025-05-17T15:52:05.406210Z",
          "shell.execute_reply.started": "2025-05-17T15:51:55.057417Z",
          "shell.execute_reply": "2025-05-17T15:52:05.405181Z"
        },
        "id": "48Z-T6bgzZYp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── MEAN POOLING (for SIDE) ───────────────────────────────────────────────\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0]\n",
        "    mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * mask, 1) / torch.clamp(mask.sum(1), min=1e-9)\n",
        "\n",
        "\n",
        "def compute_side_score(codes, hyps):\n",
        "    global side_tokenizer, side_model\n",
        "    if side_model is None:\n",
        "        checkpoint = \"/content/103080\"\n",
        "        side_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "        side_model     = AutoModel.from_pretrained(checkpoint)\n",
        "        if torch.cuda.is_available(): side_model = side_model.cuda()\n",
        "        side_model.eval()\n",
        "    scores = []\n",
        "    for code, summ in zip(codes, hyps):\n",
        "        enc = side_tokenizer([code, summ], padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        if torch.cuda.is_available(): enc = {k:v.cuda() for k,v in enc.items()}\n",
        "        with torch.no_grad(): out = side_model(**enc)\n",
        "        pooled = mean_pooling(out, enc['attention_mask'])\n",
        "        normed = torch.nn.functional.normalize(pooled, p=2, dim=1)\n",
        "        scores.append(util.pytorch_cos_sim(normed[0], normed[1]).item())\n",
        "    return round(float(np.mean(scores)),4)\n",
        "\n",
        "def compute_meteor_score(refs, hyps):\n",
        "    sc = []\n",
        "    for r,h in zip(refs, hyps):\n",
        "        rt = word_tokenize(r.lower()); ht = word_tokenize(h.lower())\n",
        "        sc.append(meteor_score([rt], ht))\n",
        "    return round(float(np.mean(sc)),4)\n",
        "\n",
        "def compute_chrf_score(refs, hyps):\n",
        "  refs = [r.lower() for r in refs]\n",
        "  hyps = [h.lower() for h in hyps]\n",
        "\n",
        "  res = sacrebleu.corpus_chrf(hyps, [refs], word_order=2)\n",
        "  return round(res.score / 100, 4)\n",
        "\n",
        "## ----BLEU METRIC-----------\n",
        "def compute_bleu_sacre(refs, hyps, lang_name):\n",
        "    lang_name = lang_name.lower()\n",
        "\n",
        "    # Define tokenizer per language\n",
        "    tokenizer_map = {\n",
        "        \"chinese\": \"zh\",\n",
        "        \"french\": \"13a\",\n",
        "        \"portuguese\": \"13a\",\n",
        "        \"arabic\": \"intl\",\n",
        "        \"hindi\": \"intl\",\n",
        "        \"spanish\": \"13a\"\n",
        "    }\n",
        "\n",
        "    # Default tokenizer if language not found\n",
        "    tokenizer = tokenizer_map.get(lang_name, \"13a\")\n",
        "\n",
        "    # Compute BLEU-4\n",
        "    score = sacrebleu.corpus_bleu(hyps, [refs], tokenize=tokenizer)\n",
        "    return round(score.score / 100, 4) # Normalize to 0–1 like nltk\n",
        "\n",
        "\n",
        "\n",
        "def tokenize(text, lang):\n",
        "    lang = lang.lower()\n",
        "    if lang == \"chinese\":\n",
        "        return list(text.strip())\n",
        "    elif lang == \"arabic\":\n",
        "        return simple_word_tokenize(text)\n",
        "    elif lang == \"hindi\":\n",
        "        return trivial_tokenize(text, lang='hi')\n",
        "    elif lang in [\"french\", \"portuguese\"]:\n",
        "        return word_tokenize(text, language=lang)\n",
        "    else:\n",
        "        return text.strip().split()\n",
        "\n",
        "\n",
        "def compute_bleu_nltk(refs_tokenized, hyps_tokenized):\n",
        "    smoothie = SmoothingFunction().method1\n",
        "    score = corpus_bleu(\n",
        "        refs_tokenized,\n",
        "        hyps_tokenized,\n",
        "        weights=(0.25, 0.25, 0.25, 0.25),\n",
        "        smoothing_function=smoothie\n",
        "    )\n",
        "    return round(score, 4)\n",
        "\n",
        "## ----COMET METRIC-----------\n",
        "\n",
        "comet = evaluate_load(\"comet\", config_name=\"Unbabel/wmt22-comet-da\")\n",
        "\n",
        "def compute_comet_score(sources, references, hypotheses, batch_size=8, gpus=0):\n",
        "    result = comet.compute(\n",
        "        sources=sources,\n",
        "        predictions=hypotheses,\n",
        "        references=references,\n",
        "    )\n",
        "    per_example = result.get(\"scores\", [])\n",
        "    mean_score = float(np.mean(per_example)) if per_example else 0.0\n",
        "    return round(mean_score, 4), per_example\n",
        "\n",
        "\n",
        "# COMPUTE ALL METRICS\n",
        "def compute_all_metrics(codes, refs, hyps, lang_name, code_lang):\n",
        "    print(f\"  Computing backtranslation-based metrics for {lang_name}...\")\n",
        "    bt = [bt_function(h, lang_name) for h in hyps]\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    refs_tokenized = [[tokenize(r, lang_name)] for r in refs]\n",
        "    hyps_tokenized = [tokenize(b, lang_name) for b in bt]\n",
        "\n",
        "    # Compute BLEU using tokenized inputs\n",
        "    bleu_nltk = compute_bleu_nltk(refs_tokenized, hyps_tokenized)\n",
        "\n",
        "    bleu_sacre = compute_bleu_sacre(refs, bt, lang_name)\n",
        "    bleu_diff = round(abs(bleu_nltk - bleu_sacre), 4)\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    rl = [scorer.score(r, b)['rougeL'].fmeasure for r, b in zip(refs, bt)]\n",
        "    comet_mean, comet_per_example = compute_comet_score(\n",
        "        sources=hyps,\n",
        "        references=refs,\n",
        "        hypotheses=bt\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"bleu4_nltk\": round(bleu_nltk, 4),\n",
        "        \"bleu4_sacrebleu\": bleu_sacre,\n",
        "        \"bleu4_diff\": bleu_diff,\n",
        "        \"rougeL\": round(np.mean(rl), 4),\n",
        "        \"meteor\": compute_meteor_score(refs, bt),\n",
        "        \"chrf++\": compute_chrf_score(refs, bt),\n",
        "        \"side_bt\": compute_side_score(codes, bt),\n",
        "        \"comet_mean\": comet_mean,\n",
        "        \"comet_per_example\": comet_per_example\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T16:01:07.499915Z",
          "iopub.execute_input": "2025-05-17T16:01:07.500259Z"
        },
        "id": "FLA_b7_XzZYq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── BACKTRANSLATION SANITY TEST ─────────────────────────────────────────────\n",
        "print(\"\\n🔍 Running backtranslation test...\")\n",
        "\n",
        "sample_inputs = {\n",
        "    \"Chinese\": \"我喜欢自然语言处理。\",\n",
        "    \"French\": \"J'aime le traitement automatique des langues.\",\n",
        "    \"Arabic\": \"أنا أحب معالجة اللغة الطبيعية.\",\n",
        "}\n",
        "\n",
        "for lang_name, input_text in sample_inputs.items():\n",
        "    print(f\"\\n🌐 {lang_name} Input: {input_text}\")\n",
        "    output = bt_function(input_text, lang_name)\n",
        "    print(f\"📝 Backtranslated Output: {output}\")\n"
      ],
      "metadata": {
        "id": "yzRAR5lCh7mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "def insert_backtranslations(data):\n",
        "    for entry in data:\n",
        "        new_entry = OrderedDict()\n",
        "        for key, value in entry.items():\n",
        "            new_entry[key] = value\n",
        "            if key.startswith(\"summary_\"):\n",
        "                lang_code = key.replace(\"summary_\", \"\").lower()\n",
        "                if lang_code in json_field_to_lang:\n",
        "                    lang_name = json_field_to_lang[lang_code]\n",
        "                    gen = entry.get(key, \"\").strip()\n",
        "                    if gen:\n",
        "                        bt_key = f\"bt_{lang_code}\"\n",
        "                        new_entry[bt_key] = sanitize_text(bt_function(gen, lang_name))\n",
        "        entry.clear()\n",
        "        entry.update(new_entry)\n",
        "\n"
      ],
      "metadata": {
        "id": "PHfUWZpK9p3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "km-X3KQa3nY9"
      },
      "cell_type": "code",
      "source": [
        "# ─── MAIN EVALUATION ─────────────────────────────────────\n",
        "def run_evaluation():\n",
        "    all_results = []\n",
        "    base_dir = \"/content/nueva/prompt_analysis\"\n",
        "    model_folders = [\"codegemma\", \"gemma-2-9b-it\", \"qwen2.5coder\", \"deepseekcoder\"]\n",
        "    prompt_subdirs = [\"prompt0\"]\n",
        "\n",
        "    bt_json_dir = \"backtranslated_jsons\"\n",
        "    os.makedirs(bt_json_dir, exist_ok=True)\n",
        "\n",
        "    for model_folder in model_folders:\n",
        "        for prompt in prompt_subdirs:\n",
        "            prompt_path = os.path.join(base_dir, model_folder, prompt)\n",
        "            if not os.path.isdir(prompt_path):\n",
        "                continue\n",
        "\n",
        "            for fname in os.listdir(prompt_path):\n",
        "                if not fname.endswith(\".json\") or not fname.startswith(\"all_languages_prompt\"):\n",
        "                    continue\n",
        "\n",
        "                summary_path = os.path.join(prompt_path, fname)\n",
        "                print(f\"\\nProcessing file: {summary_path}\")\n",
        "\n",
        "                with open(summary_path, encoding='utf-8') as f:\n",
        "                    data = json.load(f)\n",
        "\n",
        "                if not data:\n",
        "                    print(\"  Skipped: empty file\")\n",
        "                    continue\n",
        "\n",
        "                codes = [d.get(\"code\", \"\") for d in data]\n",
        "                refs = [sanitize_text(d.get(\"summary_english\", d.get(\"docstring\", \"\"))) for d in data]\n",
        "                code_lang = data[0].get(\"language\", \"unknown\")\n",
        "                model_name = data[0].get(\"model_name\", model_folder)\n",
        "                prompt_used = data[0].get(\"prompt_used\", prompt)\n",
        "\n",
        "                for field, lang_name in json_field_to_lang.items():\n",
        "                    hyp_key = f\"summary_{field}\"\n",
        "                    if hyp_key not in data[0]:\n",
        "                        print(f\"  Skipping {lang_name} — {hyp_key} not found.\")\n",
        "                        continue\n",
        "\n",
        "                    hyps = [sanitize_text(d.get(hyp_key, \"\")) for d in data]\n",
        "                    if not any(hyps):\n",
        "                        print(f\"  Skipping {lang_name} — all summaries empty.\")\n",
        "                        continue\n",
        "\n",
        "                    print(f\"  → Evaluating summaries in {lang_name}...\")\n",
        "                    bert = compute_bertscore(refs, hyps)\n",
        "                    side_original = compute_side_score(codes, hyps)\n",
        "                    metrics = compute_all_metrics(codes, refs, hyps, lang_name, code_lang)\n",
        "                    side_drop = round(side_original - metrics[\"side_bt\"], 4)\n",
        "\n",
        "                    for i, entry in enumerate(data):\n",
        "                        code = entry.get(\"code\", \"\")\n",
        "                        sample_id = entry.get(\"id\", f\"{code_lang}_{i}\")\n",
        "                        full_func = entry.get(\"whole_func_string\", code)\n",
        "                        word_len = len(full_func.strip().split())\n",
        "\n",
        "                        generated_summary = sanitize_text(entry.get(hyp_key, \"\"))\n",
        "                        backtranslated_summary = sanitize_text(bt_function(generated_summary, lang_name))\n",
        "                        reference_summary = sanitize_text(entry.get(\"summary_english\", entry.get(\"docstring\", \"\")))\n",
        "\n",
        "                        result = {\n",
        "                            \"sample_id\": sample_id,\n",
        "                            \"model_folder_name\": model_folder,\n",
        "                            \"model_name\": model_name,\n",
        "                            \"programming_language\": code_lang,\n",
        "                            \"language\": lang_name,\n",
        "                            \"prompt_used\": prompt_used,\n",
        "                            \"bt_model\": bt_model_tag,\n",
        "                            \"word_len\": word_len,\n",
        "                            \"length_bucket\": entry.get(\"length_bucket\", \"unknown\"),\n",
        "                            \"reference_summary\": reference_summary,\n",
        "                            \"generated_summary\": generated_summary,\n",
        "                            \"backtranslated_summary\": backtranslated_summary,\n",
        "                            \"bertscore_f1\": bert[\"f1\"],\n",
        "                            \"bertscore_precision\": bert[\"precision\"],\n",
        "                            \"bertscore_recall\": bert[\"recall\"],\n",
        "                            \"side_original\": side_original,\n",
        "                            \"side_bt\": metrics[\"side_bt\"],\n",
        "                            \"side_drop\": side_drop,\n",
        "                            \"bleu4_nltk\": metrics[\"bleu4_nltk\"],\n",
        "                            \"bleu4_sacrebleu\": metrics[\"bleu4_sacrebleu\"],\n",
        "                            \"bleu4_diff\": metrics[\"bleu4_diff\"],\n",
        "                            \"rougeL\": metrics[\"rougeL\"],\n",
        "                            \"meteor\": metrics[\"meteor\"],\n",
        "                            \"chrf++\": metrics[\"chrf++\"],\n",
        "                            \"comet_mean\": metrics[\"comet_mean\"],\n",
        "                            \"comet_example_score\": metrics[\"comet_per_example\"][i] if i < len(metrics[\"comet_per_example\"]) else None\n",
        "                        }\n",
        "\n",
        "                        all_results.append(result)\n",
        "\n",
        "                # Insert backtranslations into data and save\n",
        "                insert_backtranslations(data)\n",
        "                enhanced_fname = os.path.basename(summary_path).replace(\".json\", f\"_with_bt_{bt_model_tag}.json\")\n",
        "                enhanced_fpath = os.path.join(bt_json_dir, enhanced_fname)\n",
        "                with open(enhanced_fpath, \"w\", encoding='utf-8') as f:\n",
        "                    json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Save metric results\n",
        "    os.makedirs(backtranslation_dir, exist_ok=True)\n",
        "    json_out = os.path.join(backtranslation_dir, f\"all_scores_bt_{bt_model_tag}.json\")\n",
        "    csv_out = os.path.join(backtranslation_dir, f\"all_scores_bt_{bt_model_tag}.csv\")\n",
        "\n",
        "    with open(json_out, \"w\", encoding='utf-8') as f:\n",
        "        json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    pd.DataFrame(all_results).to_csv(csv_out, index=False)\n",
        "\n",
        "    print(f\"\\nSaved results to:\\n  JSON: {json_out}\\n  CSV:  {csv_out}\")\n",
        "\n",
        "\n",
        "\n",
        "run_evaluation()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Paths to the generated files\n",
        "json_out = os.path.join(backtranslation_dir, f\"all_scores_bt_{bt_model_tag}.json\")\n",
        "csv_out = os.path.join(backtranslation_dir, f\"all_scores_bt_{bt_model_tag}.csv\")\n",
        "\n",
        "# Download the files\n",
        "files.download(json_out)\n",
        "files.download(csv_out)\n"
      ],
      "metadata": {
        "id": "18C0EjtvqxKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SDlQwkUYqr3h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}